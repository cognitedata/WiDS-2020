{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAXnMj5E9jWG"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cognitedata/WiDS-2019/blob/master/WiDS_2019_Cognite_Interact_with_Assets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Using Classification Methods to Label Industrial Data\n",
    "\n",
    "## What this notebook will achieve\n",
    "\n",
    "* Extract data from an oil rig in the North Sea.\n",
    "* Add more later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "* Having a basic understanding of Python concepts will help to understand the process.\n",
    "\n",
    "* Cognite has released *live* data to the public on the Cognite Data Platform streaming from [Valhall](https://www.akerbp.com/en/our-assets/production/valhall/), one of Aker's oil fields.\n",
    "\n",
    "* To access the data, generate an API Key on [Open Industrial Data](https://openindustrialdata.com/). Get your key via the Google Access platform. You will be asked to fill out some personal information to generate your personal key.\n",
    "\n",
    "* Visualize some of the machines (assets) on Valhall with Cognite's [Operational Intelligence](https://opint.cogniteapp.com/publicdata/infographics/-LOHKEJPLvt0eRIZu8mE) dashboard. This data on this page shows is streaming live data from the Valhall oil field located in the North Sea.\n",
    "\n",
    "* To understand how to interact with the data using the Python SDK ([Docs](https://cognite-sdk-python.readthedocs-hosted.com/en/latest/)) follow along in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8-Oj493ReNU"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Lf5pnaDBLnc"
   },
   "source": [
    "#### Install the Cognite SDK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "pbgkP3p59iiK",
    "outputId": "c8220cde-0930-414d-8d53-231b1052d15b"
   },
   "outputs": [],
   "source": [
    "!pip install cognite-sdk\n",
    "!pip install cognite-datastudio\n",
    "!pip install scikit-learn==0.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CN5J9FmPBRLk"
   },
   "source": [
    "#### Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFkQ73a44wmU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from cognite.client import CogniteClient\n",
    "from util.helper_functions import FitCountVectorizer, FitSimilarityEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pr0CQECZBWW3"
   },
   "source": [
    "#### Connect to the Cognite Data Platform\n",
    "* This client object is how all queries will be sent to the Cognite API to retrieve data.\n",
    "\n",
    "When prompted for your API key, use the key generated by open industrial data as mentioned in the Getting Started steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E8CdoRFu9n2t",
    "outputId": "43130697-9674-407a-cac4-c871a9c82757"
   },
   "outputs": [],
   "source": [
    "client = CogniteClient(api_key=getpass(\"Open Industrial Data API-KEY: \"),\n",
    "                       project=\"publicdata\", client_name=\"OID_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKFPBwYsRjHL"
   },
   "source": [
    "## Accessing Cognite Data Platform (CDP)\n",
    "* The CDP organizes digital information about the physical world.\n",
    "* There are 6 kinds of objects stored on the CDP. Each of these objects in the CDP are labelled with a unique ID. Information regarding a specific Asset, Event, etc are often retrieved using this ID.\n",
    "\n",
    "  * [Assets](https://doc.cognitedata.com/api/0.5/#tag/Assets) are digital representations of physical objects or groups of objects, and assets are organized into an asset hierarchy. For example, an asset can represent a water pump which is part of a subsystem on an oil platform.\n",
    "  \n",
    "  * [Event](https://doc.cognitedata.com/api/0.5/#tag/Events) objects store complex information about multiple assets over a time period. For example, an event can describe two hours of maintenance on a water pump and some associated pipes.\n",
    "  \n",
    "  * A [File](https://doc.cognitedata.com/api/0.5/#tag/Files) stores a sequence of bytes connected to one or more assets. For example, a file can contain a piping and instrumentation diagram (P&IDs) showing how multiple assets are connected.\n",
    "  \n",
    "  * A [Time Series](https://doc.cognitedata.com/api/0.5/#tag/Time-series) consists of a sequence of data points connected to a single asset. For example: A water pump asset can have a temperature time series that records a data point in units of Â°C every second.\n",
    "  \n",
    "  * [Sequence](https://doc.cognitedata.com/api/0.5/#tag/Sequences) are similar to time series in that they are a key value pair, but rather than using a timestamp as the key, another measurment such as depth could be the key. For example, this is used in practice when drilling and taking measurments at various depths.\n",
    "  \n",
    "  * A [3D](https://doc.cognitedata.com/api/0.5/#tag/3D) model is typically built up by a hierarchical structure. This looks very similar to how we organize our internal asset hierarchy. 3D models are visualized via Cognite's dashboards.\n",
    "  \n",
    "* It is important to refer back to the [SDK](https://cognite-sdk-python.readthedocs-hosted.com/en/latest/cognite.html) for specific details on arguments on all avaiable methods on how to access these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fn9FD2lMWUwI"
   },
   "source": [
    "### Collecting Asset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRiusGapFmT8"
   },
   "source": [
    "#### Retrieve a list of all Assets\n",
    "\n",
    "* There are thousands of Assets in the CDP, we can have a look at a few examples.\n",
    "\n",
    "* This will generate a list of assets from the CDP with no particular filters, this is a random result. Generally we would want to apply filters when retrieving records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "_sACMOGWFmna",
    "outputId": "41e482b7-70b4-41f7-bfc9-014e77205a5c"
   },
   "outputs": [],
   "source": [
    "client.assets.list().to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysq_kFWLGYFV"
   },
   "source": [
    "#### Decide on which asset we want to explore\n",
    "* To get started exploring data in the CDP, we must first decide on which Asset we want to gather information from.\n",
    "\n",
    "* Some asset names may be retrieved from the [Op Int](https://opint.cogniteapp.com/publicdata/infographics/-LOHKEJPLvt0eRIZu8mE) dashboard.\n",
    "\n",
    "* Here is a screehshot of the [OpInt Dashboard](https://drive.google.com/open?id=1f_7nJaJu5Xgr3Oq09mIZ0KwjBAYbzEUQ) incase the page does not load.\n",
    "\n",
    "* Some example asset names are:\n",
    "  * 23-HA-9103\n",
    "  * 23-PV-92583\n",
    "  * 23-VG-9101\n",
    "  \n",
    "The *fuzzy* search for an asset can be performed as followed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "L9DHHUpwBJOv",
    "outputId": "6917abc2-a7ad-4432-ece4-d4fe42d3dc56"
   },
   "outputs": [],
   "source": [
    "asset_name = \"23-HA-9103\"\n",
    "asset_df = client.assets.search(name=asset_name).to_pandas()\n",
    "asset_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THRHdIXwS-bf"
   },
   "source": [
    "#### Get information on the asset of interest\n",
    "\n",
    "* We can filter the assets to get asset-specific details based on asset_name\n",
    "\n",
    "* The *get_asset()* interface provides the same information for 1 specific asset based on the provided ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "kxnrtYoYSBGo",
    "outputId": "f1d57fa0-30e0-4f21-ff1b-1f7c5d9d6c32"
   },
   "outputs": [],
   "source": [
    "asset_id = asset_df[asset_df[\"name\"] == asset_name].iloc[0]['id']\n",
    "asset = client.assets.retrieve(id=asset_id).to_pandas()\n",
    "asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X75GOITO_m1q"
   },
   "source": [
    "#### How do we get Asset relationships?\n",
    "\n",
    "* The interface *get_asset_subtree()* can be used to retrieve the *children* of an Asset. \n",
    "\n",
    "* Each Asset is given various properties, some of the useful ones for this method are:\n",
    "\n",
    "  * Depth: The number of edges from the parent node\n",
    "  \n",
    "  * Description: Includes information such as the platform and type of sensor being monitored\n",
    "  \n",
    "We will generate a list of all children of the main asset of interest. This is done by specifying a depth of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "1OPecRPv6xsU",
    "outputId": "aa33544c-6a3c-4cb8-f3cc-3933bb116289"
   },
   "outputs": [],
   "source": [
    "subtree_df = client.assets.retrieve_subtree(id=asset_id, depth=1).to_pandas()\n",
    "subtree_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding type/classes of assets\n",
    "\n",
    "We will now try to classify/group the assets into different types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get relevant data\n",
    "\n",
    "We will use name and description to classify/group the assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all assets\n",
    "asset_list = client.assets.list(limit=-1).to_pandas()[[\"id\", \"name\", \"description\"]]\n",
    "print(f\"There are {len(asset_list)} assets in total.\")\n",
    "asset_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_list.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labeled data\n",
    "\n",
    "We have created labels for some of the assets. We will use these labels to see how different features are able to separate the data into different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv(\"data/oid_assets_types.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assets_with_label = pd.merge(asset_list, labeled_data, how=\"left\")\n",
    "# Set type to unknown for the assets where we do not know the type.\n",
    "all_assets_with_label[\"type\"].fillna(\"unknown\", inplace = True)\n",
    "print(f\"There are {sum(all_assets_with_label['type']!='unknown')} with known type and\\\n",
    "      {sum(all_assets_with_label['type']=='unknown')} with unknown type\")\n",
    "all_assets_with_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the different types\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Count\": all_assets_with_label\n",
    "        .groupby([\"type\"])\n",
    "        .size()\n",
    "    }\n",
    ").sort_values(by=\"Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unknown for now\n",
    "assets_with_label = all_assets_with_label[all_assets_with_label[\"type\"]!=\"unknown\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features from the data\n",
    "Before we test any supervised on unsupervised algorithms, we need to create features from the data.\n",
    "\n",
    "### Basic features\n",
    "\n",
    "First we will look at two basic features:\n",
    "1. Length of the name\n",
    "2. Number of special characters in the name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Length of the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_with_label[\"length_name\"] = assets_with_label[\"name\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_with_label.groupby(\"type\")[\"length_name\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Number of special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_with_label['num_dach_name'] = [len(x.split('-')) -1 for x in assets_with_label['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_with_label.groupby(\"type\")[\"num_dach_name\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information from text/strings\n",
    "\n",
    "There are many different methods for extracting information from text. \n",
    "\n",
    "For the description we will look at how to extract all tokens from the description, and use the count of the most common tokens in each description as features.\n",
    "\n",
    "For the names we will create features from the similarity between each name and some fixed number of unique names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words/tokens in decsription\n",
    "\n",
    "The description can be seen as a very short document. We will use CountVectorizer to extract tokens from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10)\n",
    "vectorizer.fit(assets_with_label[\"description\"])\n",
    "# Get the words in the dict\n",
    "vectorizer.get_feature_names()\n",
    "count_vectorizer_desc = vectorizer.transform(assets_with_label[\"description\"])\n",
    "df_count_vectorizer_desc = pd.DataFrame(count_vectorizer_desc.toarray(),\n",
    "                                        columns = vectorizer.get_feature_names())\n",
    "df_count_vectorizer_desc[\"description\"] = assets_with_label[\"description\"]\n",
    "df_count_vectorizer_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between asset names\n",
    "\n",
    "The names of the assets might look random, but it is not. There is a lot of information about the asset in the structure. With string features we would often create one dummy feature for each unique string, but all the names of the assets are unique. \n",
    "\n",
    "We would like to capture the similarity between the names without knowing what the different letter combinations actually means.\n",
    "\n",
    "There are different methods for creating features that captures these similarities, today we will lokk at similarity encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the string\n",
    "We do not care about difference in digits and will therefore convert all numbers to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "assets_with_label[\"name_cleaned\"] = [ re.sub(r\"\\d\", \"1\", x) for x in assets_with_label[\"name\"]]\n",
    "assets_with_label[[\"name\", \"name_cleaned\"]] .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique elements before cleaning {len(set(assets_with_label['name']))}\")\n",
    "print(f\"Number of unique elements after cleaning {len(set(assets_with_label['name_cleaned']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dirty_cat import SimilarityEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialaze the similarity encoder\n",
    "similarity_encoder = SimilarityEncoder(\n",
    "similarity=\"ngram\",\n",
    "dtype=np.float32,\n",
    "categories=\"most_frequent\",\n",
    "n_prototypes=10,\n",
    "random_state=1006\n",
    ")\n",
    "    \n",
    "    #Fit the similarity encoder and transform the data\n",
    "similarity_encoder.fit(assets_with_label[\"name\"].values.reshape(-1, 1))\n",
    "sim_enc = similarity_encoder.transform(assets_with_label[\"name\"].values.reshape(-1, 1))\n",
    "sim_enc_df = pd.DataFrame(sim_enc, columns = list(similarity_encoder.categories_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_enc_df[\"name_cleaned\"] = assets_with_label[\"name_cleaned\"]\n",
    "sim_enc_df[\"type\"] = assets_with_label[\"type\"]\n",
    "sim_enc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (unsupervised classification)\n",
    "\n",
    "In a scenario where we do not have any labeled data we must used unsupervised methods such as clustering. Theree are different types of clustering. In this workshop we will use K-Means clustering to group the assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from name and description\n",
    "\n",
    "vectorizer = FitCountVectorizer(col_name=\"description\")\n",
    "vectorizer.fit(df=assets_with_label)\n",
    "count_vec_array = vectorizer.transform(df=assets_with_label)\n",
    "\n",
    "sim_enc = FitSimilarityEncoder(col_name=\"name_cleaned\")\n",
    "sim_enc.fit(df=assets_with_label)\n",
    "sim_enc_array = sim_enc.transform(df=assets_with_label)\n",
    "\n",
    "X = np.concatenate((sim_enc_array, count_vec_array), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster the data\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=1006).fit(X)\n",
    "\n",
    "\"\"\"\n",
    "Todo: This does not work well, should investigate the cluster and see if there is somthing we can add to \n",
    "get better clusters. Can also consider just clustering on some types.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_with_label[\"cluster_label\"] = kmeans.labels_\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Count\": assets_with_label\n",
    "        .groupby([\"cluster_label\", \"type\"])\n",
    "        .size()\n",
    "    }\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Given that we have some training data we can train a classification model and try to predict the type for the rest of our data. There are many different classification algorithems in this tutorial we will use K nearest neighbors.\n",
    "\n",
    "\n",
    "### Train test split\n",
    "We will train on one part of our data and test our algorithm on the second part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, y_train, y_test = train_test_split(\n",
    "    assets_with_label, assets_with_label[\"type\"].values,\n",
    "    train_size=0.7,\n",
    "    stratify=assets_with_label[\"type\"].values,\n",
    "    random_state=1006,\n",
    ")\n",
    "df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create features\n",
    "Fit on the training data and transform both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit on train\n",
    "vectorizer = FitCountVectorizer(col_name=\"description\")\n",
    "vectorizer.fit(df=df_train)\n",
    "\n",
    "sim_enc = FitSimilarityEncoder(col_name=\"name_cleaned\")\n",
    "sim_enc.fit(df=df_train)\n",
    "\n",
    "# Transform train\n",
    "count_vec_array = vectorizer.transform(df=df_train)\n",
    "sim_enc_array = sim_enc.transform(df=df_train)\n",
    "X_train = np.concatenate((sim_enc_array, count_vec_array), axis=1)\n",
    "\n",
    "#Transform test\n",
    "count_vec_array = vectorizer.transform(df=df_test)\n",
    "sim_enc_array = sim_enc.transform(df=df_test)\n",
    "X_test = np.concatenate((sim_enc_array, count_vec_array), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X=X_train, y=y_train)\n",
    "KNeighborsClassifier(...)\n",
    "print(neigh.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(neigh.predict(X_test)== y_test)/len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open set classification\n",
    "\n",
    "### Motivation\n",
    "In the situation above we knew in advance all the classes we were interested in and we also had examples of all the classes we were interested in. Let's see what happens if we for instance remove all the \"alarm\" examples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a situation where we do not know all the classes at training time we need an algorithm that not only correctly predicts the class of an item, but that is also able to return an unknown label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognite.datastudio.resource_typing import ResourceTyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data on correct format\n",
    "training_data = []\n",
    "for i, target in enumerate(y_train):\n",
    "    training_data.append({\"data\": list(df_train.loc[i, [\"name\", \"description\"]]), \"target\": target})\n",
    "    \n",
    "predict_data = []\n",
    "for i in enumerate(y_test):\n",
    "    predict_data.append({\"data\": list(df_test.loc[i, [\"name\", \"description\"]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = ResourceTyping(client)\n",
    "model = matcher.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = model.predict(predict_data)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Introduction_to_Data_Visualization_CDP.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
